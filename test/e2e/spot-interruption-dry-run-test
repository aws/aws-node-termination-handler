#!/bin/bash
set -euo pipefail

# Available env vars:
#   $TMP_DIR
#   $CLUSTER_NAME
#   $KUBECONFIG
#   $NODE_TERMINATION_HANDLER_DOCKER_REPO
#   $NODE_TERMINATION_HANDLER_DOCKER_TAG
#   $EC2_METADATA_DOCKER_REPO
#   $EC2_METADATA_DOCKER_TAG

echo "Starting Maintenance Events Dry-Run Test for Node Termination Handler"

SCRIPTPATH="$( cd "$(dirname "$0")" ; pwd -P )"

helm upgrade --install $CLUSTER_NAME-anth $SCRIPTPATH/../../config/helm/aws-node-termination-handler/ \
  --wait \
  --force \
  --namespace kube-system \
  --set instanceMetadataURL="http://localhost:1342" \
  --set image.repository="$NODE_TERMINATION_HANDLER_DOCKER_REPO" \
  --set image.tag="$NODE_TERMINATION_HANDLER_DOCKER_TAG" \
  --set dryRun="true" \
  --set enableSpotInterruptionDraining="true" \
  --set enableScheduledEventDraining="true"

helm upgrade --install $CLUSTER_NAME-emtp $SCRIPTPATH/../../config/helm/ec2-metadata-test-proxy/ \
  --wait \
  --force \
  --namespace default \
  --set ec2MetadataTestProxy.image.repository="$EC2_METADATA_DOCKER_REPO" \
  --set ec2MetadataTestProxy.image.tag="$EC2_METADATA_DOCKER_TAG" \
  --set ec2MetadataTestProxy.port=1342

POD_ID=$(kubectl get pods --namespace kube-system | grep -i node-termination-handler | grep Running | cut -d' ' -f1)

for i in $(seq 0 10); do
  if [[ ! -z $(kubectl logs $POD_ID -n kube-system | grep -i -e 'would have been cordoned and drained') ]]; then
      echo "✅ Verified the dryrun logs were executed"
      if kubectl get nodes $CLUSTER_NAME-worker --no-headers | grep -v SchedulingDisabled; then
          echo "✅ Verified the worker node was not cordoned!"
          echo "✅ Spot Interruption Dry Run Test Passed $CLUSTER_NAME! ✅"
          exit 0
      fi
  fi
  sleep 10
done

exit 1
