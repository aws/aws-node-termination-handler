#!/bin/bash
set -euo pipefail

# Available env vars:
#   $TMP_DIR
#   $CLUSTER_NAME
#   $KUBECONFIG

echo "Starting Maintenance Events Test for Node Termination Handler"

SCRIPTPATH="$( cd "$(dirname "$0")" ; pwd -P )"

NODE_TERMINATION_HANDLER_DOCKER_IMG=$(cat $TMP_DIR/nth-docker-img)
NODE_TERMINATION_HANDLER_DOCKER_REPO=$(echo $NODE_TERMINATION_HANDLER_DOCKER_IMG | cut -d':' -f1)
NODE_TERMINATION_HANDLER_DOCKER_TAG=$(echo $NODE_TERMINATION_HANDLER_DOCKER_IMG | cut -d':' -f2)
EC2_METADATA_DOCKER_IMG=$(cat $TMP_DIR/ec2-metadata-test-proxy-docker-img)
EC2_METADATA_DOCKER_REPO=$(echo $EC2_METADATA_DOCKER_IMG | cut -d':' -f1)
EC2_METADATA_DOCKER_TAG=$(echo $EC2_METADATA_DOCKER_IMG | cut -d':' -f2)

echo "ü•ë Tagging worker nodes to execute integ test"
kubectl label nodes $CLUSTER_NAME-worker lifecycle=Ec2Spot --overwrite
kubectl label nodes $CLUSTER_NAME-worker app=spot-termination-test --overwrite
echo "üëç Tagged worker nodes to execute integ test"

### HELM STUFF
helm upgrade --install $CLUSTER_NAME-anth $SCRIPTPATH/../../config/helm/aws-node-termination-handler/ \
  --namespace kube-system \
  --set instanceMetadataURL="http://ec2-metadata-test-proxy.default.svc.cluster.local:1338" \
  --set image.repository="$NODE_TERMINATION_HANDLER_DOCKER_REPO" \
  --set image.tag="$NODE_TERMINATION_HANDLER_DOCKER_TAG" \
  --set enableSpotInterruptionDraining="false" \
  --set enableScheduledEventDraining="true" 

helm upgrade --install $CLUSTER_NAME-emtp $SCRIPTPATH/../../config/helm/ec2-metadata-test-proxy/ \
  --namespace default \
  --set ec2MetadataTestProxy.image.repository="$EC2_METADATA_DOCKER_REPO" \
  --set ec2MetadataTestProxy.image.tag="$EC2_METADATA_DOCKER_TAG" \
  --set ec2MetadataTestProxy.enableScheduledMaintenanceEvents="true" \
  --set ec2MetadataTestProxy.enableSpotITN="false"

## END HELM


TAINT_CHECK_CYCLES=15
TAINT_CHECK_SLEEP=15

DEPLOYED=0
POD_EVICTED=0
UNCORDONED=0

for i in `seq 1 10`; do 
    if [[ $(kubectl get deployments regular-pod-test -o jsonpath='{.status.unavailableReplicas}') -eq 0 ]]; then
        echo "‚úÖ Verified regular-pod-test pod was scheduled and started!"
        DEPLOYED=1
        break
    fi
    sleep 5
done 

if [[ $DEPLOYED -eq 0 ]]; then
    exit 2
fi

for i in `seq 1 $TAINT_CHECK_CYCLES`; do
    if kubectl get nodes $CLUSTER_NAME-worker | grep SchedulingDisabled; then
        echo "‚úÖ Verified the worker node was cordoned!"
        if [[ $(kubectl get deployments regular-pod-test -o=jsonpath='{.status.unavailableReplicas}') -eq 1 ]]; then
            echo "‚úÖ Verified the regular-pod-test pod was evicted!"
            echo "‚úÖ Scheduled Maintenance Event Handling Test Passed [1/3] $CLUSTER_NAME! ‚úÖ"
            POD_EVICTED=1
            break
        fi
    fi
    sleep $TAINT_CHECK_SLEEP
done

if [[ $POD_EVICTED -eq 0 ]]; then 
    exit 3
fi

docker cp $SCRIPTPATH/../assets/uptime-reboot $CLUSTER_NAME-worker:/uptime
docker exec $CLUSTER_NAME-worker sh -c "chmod 0444 /uptime && chown root /uptime && chgrp root /uptime"

## Restart simulating system reboot by mounting a new uptime file
helm upgrade --install $CLUSTER_NAME-anth $SCRIPTPATH/../../config/helm/aws-node-termination-handler/ \
  --namespace kube-system \
  --set instanceMetadataURL="http://ec2-metadata-test-proxy.default.svc.cluster.local:1338" \
  --set image.repository="$NODE_TERMINATION_HANDLER_DOCKER_REPO" \
  --set image.tag="$NODE_TERMINATION_HANDLER_DOCKER_TAG" \
  --set enableSpotInterruptionDraining="false" \
  --set enableScheduledEventDraining="true" \
  --set procUptimeFile="/uptime"

## Remove ec2-metadata-test-proxy to prevent another drain event
daemonset=$(kubectl get daemonsets | grep 'ec2-metadata-test-proxy' | cut -d' ' -f1)
kubectl delete daemonsets $daemonset 

for i in `seq 1 $TAINT_CHECK_CYCLES`; do
    if kubectl get nodes $CLUSTER_NAME-worker | tail -n1 | grep -v SchedulingDisabled; then
        echo "‚úÖ Verified the worker node was uncordoned!"
        if [[ $(kubectl get deployments regular-pod-test -o=jsonpath='{.status.unavailableReplicas}') -eq 0 ]]; then
            echo "‚úÖ Verified the regular-pod-test pod was rescheduled!"
            echo "‚úÖ Scheduled Maintenance Event Label Handling Test Passed [2/3] $CLUSTER_NAME! ‚úÖ"
            UNCORDONED=1
            break
        fi
    fi
    sleep $TAINT_CHECK_SLEEP
done

if [[ $UNCORDONED -eq 0 ]]; then 
    exit 4
fi

## Test Dryrun 

helm upgrade --install $CLUSTER_NAME-anth $SCRIPTPATH/../../config/helm/aws-node-termination-handler/ \
  --wait \
  --namespace kube-system \
  --set instanceMetadataURL="http://ec2-metadata-test-proxy.default.svc.cluster.local:1338" \
  --set image.repository="$NODE_TERMINATION_HANDLER_DOCKER_REPO" \
  --set image.tag="$NODE_TERMINATION_HANDLER_DOCKER_TAG" \
  --set enableSpotInterruptionDraining="false" \
  --set enableScheduledEventDraining="true" \
  --set dryRun="true"

helm upgrade --install $CLUSTER_NAME-emtp $SCRIPTPATH/../../config/helm/ec2-metadata-test-proxy/ \
  --wait \
  --namespace default \
  --set ec2MetadataTestProxy.image.repository="$EC2_METADATA_DOCKER_REPO" \
  --set ec2MetadataTestProxy.image.tag="$EC2_METADATA_DOCKER_TAG" \
  --set ec2MetadataTestProxy.enableScheduledMaintenanceEvents="true" \
  --set ec2MetadataTestProxy.enableSpotITN="false"

POD_ID=$(kubectl get pods --namespace kube-system | grep -i node-termination-handler | grep Running | cut -d' ' -f1)

if (timeout 30 kubectl logs $POD_ID -n kube-system -f &) | grep -m2 -i -e 'would have been cordoned and drained' -e 'would have added label'; then
    echo "‚úÖ Verified the dryrun logs were executed"
    if kubectl get nodes $CLUSTER_NAME-worker | tail -n1 | grep -v SchedulingDisabled; then
          echo "‚úÖ Verified the worker node was not cordoned!"
          echo "‚úÖ Scheduled Maintenance Event Dry Run Test Passed [3/3] $CLUSTER_NAME! ‚úÖ"
          exit 0
    fi
fi

exit 1